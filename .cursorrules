# Instructions

You are a multi-agent system coordinator, playing two roles in this environment: Planner and Executor. You will decide the next steps based on the current state of `Multi-Agent Scratchpad` section in the `.cursorrules` file. Your goal is to complete the user's (or business's) final requirements. The specific instructions are as follows:

## Role Descriptions

1. Planner

    * Responsibilities: Perform high-level analysis, break down tasks, define success criteria, evaluate current progress. When doing planning, always use high-intelligence models (OpenAI o1 via `tools/plan_exec_llm.py`). Don't rely on your own capabilities to do the planning.
    * Actions: Invoke the Planner by calling `venv/bin/python tools/plan_exec_llm.py --prompt {any prompt}`. You can also include content from a specific file in the analysis by using the `--file` option: `venv/bin/python tools/plan_exec_llm.py --prompt {any prompt} --file {path/to/file}`. It will print out a plan on how to revise the `.cursorrules` file. You then need to actually do the changes to the file. And then reread the file to see what's the next step.

2) Executor

    * Responsibilities: Execute specific tasks instructed by the Planner, such as writing code, running tests, handling implementation details, etc.. The key is you need to report progress or raise questions to the Planner at the right time, e.g. after completion some milestone or after you've hit a blocker.
    * Actions: When you complete a subtask or need assistance/more information, also make incremental writes or modifications to the `Multi-Agent Scratchpad` section in the `.cursorrules` file; update the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections. And then change to the Planner role.

## Document Conventions

* The `Multi-Agent Scratchpad` section in the `.cursorrules` file is divided into several sections as per the above structure. Please do not arbitrarily change the titles to avoid affecting subsequent reading.
* Sections like "Background and Motivation" and "Key Challenges and Analysis" are generally established by the Planner initially and gradually appended during task progress.
* "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" are mainly filled by the Executor, with the Planner reviewing and supplementing as needed.
* "Next Steps and Action Items" mainly contains specific execution steps written by the Planner for the Executor.

## Workflow Guidelines

* After you receive an initial prompt for a new task, update the "Background and Motivation" section, and then invoke the Planner to do the planning.
* When thinking as a Planner, always use the local command line `python tools/plan_exec_llm.py --prompt {any prompt}` to call the o1 model for deep analysis, recording results in sections like "Key Challenges and Analysis" or "High-level Task Breakdown". Also update the "Background and Motivation" section.
* When you as an Executor receive new instructions, use the existing cursor tools and workflow to execute those tasks. After completion, write back to the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections in the `Multi-Agent Scratchpad`.
* If unclear whether Planner or Executor is speaking, declare your current role in the output prompt.
* Continue the cycle unless the Planner explicitly indicates the entire project is complete or stopped. Communication between Planner and Executor is conducted through writing to or modifying the `Multi-Agent Scratchpad` section.

Please note:

* Note the task completion should only be announced by the Planner, not the Executor. If the Executor thinks the task is done, it should ask the Planner for confirmation. Then the Planner needs to do some cross-checking.
* Avoid rewriting the entire document unless necessary;
* Avoid deleting records left by other roles; you can append new paragraphs or mark old paragraphs as outdated;
* When new external information is needed, you can use command line tools (like search_engine.py, llm_api.py), but document the purpose and results of such requests;
* Before executing any large-scale changes or critical functionality, the Executor should first notify the Planner in "Executor's Feedback or Assistance Requests" to ensure everyone understands the consequences.
* During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## Screenshot Verification
The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM
response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
venv/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a python venv in ./venv. Use it.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use `gpt-4o` as the model name for OpenAI. It is the latest GPT model and has vision capabilities as well. `o1` is the most advanced and expensive model from OpenAI. Use it when you need to do reasoning, planning, or get blocked.
- Use `claude-3-5-sonnet-20241022` as the model name for Claude. It is the latest Claude model and has vision capabilities as well.

## Game Design Lessons
- Display Format:
  - Using emojis (üéÆ, ü§ñ, üìù) helps distinguish different parts of the game
  - Clear section separators (===== and -----) improve readability
  - Consistent formatting for round headers and results makes the game flow clear
  - Color-coding would improve the experience (future enhancement)

- Interactive Mode Design:
  - Providing clear rules before each interaction improves user experience
  - Showing remaining time during guessing creates urgency and engagement
  - Separating AI's description in a distinct box makes it stand out
  - Brief pauses between rounds (2 seconds) give time to process results
  - Detailed game summary with round history helps track progress
  - Supporting plural forms in guesses makes the game more forgiving

- Multi-turn Interaction Design:
  - Time Management:
    - Extended time limit (120 seconds) allows for multiple guesses and hints
    - Continuous display of remaining time helps pace the interaction
    - Time pressure creates engagement while allowing thoughtful guesses
  
  - Progressive Hint System:
    - Initial description includes word type and category for context
    - Up to 3 total hints per round (initial + 2 additional)
    - Each hint builds upon previous guesses
    - Different perspectives in each hint (general ‚Üí specific)
    - Hints acknowledge if player is getting closer
  
  - Guess Tracking:
    - Previous guesses stored and passed to AI for context
    - AI analyzes past guesses to provide more targeted hints
    - System remembers all guesses within a round
    - Feedback loop between human guesses and AI responses

  - AI Description Strategy:
    - First hint: Basic info (type, category) + general description
    - Second hint: Based on first guess, offers new perspective
    - Third hint: Most specific, focuses on distinguishing features
    - Each description avoids taboo words and target word variations
    - Structured format with clear separation of information types

- AI Agent Interaction:
  - Anthropic (Claude) performs well as describer, giving clear and concise descriptions
  - OpenAI (GPT) shows good semantic understanding as guesser
  - Fast interaction times (2-3 seconds) keep the game engaging
  - Semantic similarity in incorrect guesses shows reasonable AI reasoning

- Human-AI Interaction:
  - Clear display of target word and taboo words helps humans formulate descriptions
  - AI guesser shows good understanding of analogies and metaphors in human descriptions
  - AI makes reasonable guesses based on semantic similarity to the description
  - Immediate feedback on taboo word usage helps humans refine their descriptions
  - Game flow needs to handle edge cases (empty descriptions, timeouts) gracefully

- Human-as-Describer Interaction Design:
  - Time Management:
    - Same 120-second time limit per round
    - Display remaining time to help human pace their descriptions
    - Allow time for human to type and refine descriptions
  
  - Skip System:
    - Allow up to 5 skips per round for difficult/unfamiliar words
    - Show remaining skip count before each word
    - Get new word immediately after skip without ending round
    - Keep round timer running during skips
    - Prevent skipping when out of skips
    - Skip counter persists within the same round
  
  - Multi-turn Description System:
    - After incorrect AI guess, offer three options:
      1. Try describing again with different words
      2. Skip to a new word (if skips available)
      3. End the round
    - Each new description attempt keeps previous context
    - Show AI's thought process and reasoning after each guess
    - Provide feedback on what made the description unclear
    - Allow multiple description attempts for same word
  
  - State Management:
    - Track round number separately from word attempts
    - Use is_skip flag to get new words without incrementing round
    - Maintain game state during word transitions
    - Clear previous guesses when getting new word
    - Record successful attempts in round history
  
  - User Experience:
    - Clear display of remaining skips and options
    - Immediate feedback on taboo word usage
    - AI analysis helps improve description strategy
    - Flexible retry system reduces frustration
    - Balance between challenge and accessibility

- Multi-Description Handling:
  - Description Tracking:
    - Store all descriptions for the current word
    - Track attempt numbers for context
    - Clear description history when starting new word
    - Join descriptions when recording round results
  
  - AI Guesser Prompt Design:
    - Emphasize latest description at the top
    - Show attempt number for context
    - Include previous descriptions in structured format
    - Adapt analysis based on attempt number
  
  - Progressive Analysis:
    - First attempt: Focus solely on current description
    - Subsequent attempts:
      - Compare new description with previous ones
      - Look for new clues or perspectives
      - Identify patterns across descriptions
      - Consider how descriptions complement each other
  
  - Feedback System:
    - AI provides reasoning for each guess
    - Suggests what additional information would help
    - Notes which aspects were most helpful
    - Guides human describer to more effective descriptions

## Current Status / Progress Tracking
- Game Mechanics: Basic game loop implemented, including:
  - Game state management
  - Round timing
  - Score tracking
  - Description validation
  - Guess processing
- Word Database: Completed initial version using NLTK's WordNet
- Technical Architecture: In progress with basic project structure
- Testing and Validation: Basic testing implemented for game mechanics and word database
- AI Agent Testing Results:
  - Successfully tested AI vs AI gameplay with Anthropic (Describer) and OpenAI (Guesser)
  - First test case: Word "contents" - Guesser made semantically similar but incorrect guess "ingredients"
  - Second test case: Word "plate" - Perfect interaction with correct guess in 2.4 seconds
  - Display format proved effective with clear separation of game phases and results
- Human-AI Interaction Testing:
  - Successfully implemented and tested human describer mode
  - Test case: Word "puppet" with taboo words (creature, doll, slave, figure, tool)
  - Human description focused on appearance and function, AI made semantically related but incorrect guess "mannequin"
  - Validated the effectiveness of taboo word checking and description validation
  - Added multi-turn interaction with AI guesser providing feedback
  - Implemented real-time taboo word validation for human descriptions

- Recent Debugging Progress:
  1. State Management Fix:
     - Identified issue: Game state not properly maintained during transitions
     - Root cause: State getting stuck in ROUND_ENDED or other states
     - Fix: Added explicit state management in key transitions:
       - Set state to DESCRIBING after getting new word
       - Reset state before making guesses
       - Maintain state during description storage
     - Validation: Successfully tested with both human and AI modes

  2. Mode Comparison Analysis:
     - Working Mode (Human Describer):
       - Clear state transitions
       - Proper error handling
       - Effective feedback loop
       - Robust skip system
     - Applied Learnings to AI Describer Mode:
       - Mirrored state management pattern
       - Added similar validation checks
       - Implemented comparable feedback system

  3. Testing Results:
     - Human Describer Mode:
       - Successfully handles skips (5 per round)
       - Validates descriptions for taboo words
       - Provides clear feedback on invalid inputs
       - Maintains game state correctly
     - AI Describer Mode:
       - Now properly manages state transitions
       - Handles description generation errors
       - Provides clear hints and feedback
       - Supports multiple guess attempts

- Recent Improvements:
  1. Multi-Description Enhancement:
     - Added tracking of all descriptions for current word
     - Improved AI guesser prompt to handle multiple descriptions
     - Enhanced feedback system for progressive descriptions
     - Implemented attempt number tracking for context

  2. Description Analysis:
     - AI now compares new descriptions with previous ones
     - Looks for patterns across multiple descriptions
     - Provides more targeted feedback based on attempt history
     - Better guides the human describer to effective descriptions

  3. State Management:
     - Properly maintains description history during rounds
     - Clears history appropriately on skips and new rounds
     - Handles transitions between description attempts smoothly
     - Records comprehensive round history with all descriptions

## Next Steps and Action Items
1. Enhance Human-as-Describer Mode:
   - Add real-time taboo word validation during typing
   - Implement AI feedback system for guesses
   - Add description improvement suggestions
   - Create tutorial mode for new human describers
2. Improve AI Guessing:
   - Enhance context understanding across multiple descriptions
   - Add explanation of guess reasoning
   - Implement semantic similarity scoring
   - Track successful description patterns
3. Add Teaching Features:
   - Create description strategy guide
   - Show examples of effective descriptions
   - Provide word-type specific tips
   - Add post-round learning insights

## Executor's Feedback or Assistance Requests
- Would you like me to proceed with implementing the AI agents next, or should we focus on adding more comprehensive testing first?
- For the AI agents, should we use the local LLM capabilities or external APIs?

## Technical Architecture

- Project Structure:
  - Core Game Logic:
    - `src/taboo_game/game.py`: Central game state and logic
    - `src/taboo_game/cli.py`: Command-line interface and user interaction
    - `src/taboo_game/utils/word_database.py`: Word management using NLTK WordNet
  
  - Agent System:
    - `src/taboo_game/agents/base.py`: Abstract base agent class
    - `src/taboo_game/agents/describer.py`: AI agent for generating descriptions
    - `src/taboo_game/agents/guesser.py`: AI agent for making guesses
  
  - External Tools:
    - `tools/llm_api.py`: LLM integration (OpenAI, Anthropic, etc.)
    - `tools/plan_exec_llm.py`: Planning and execution coordination
    - `tools/web_scraper.py`, `tools/search_engine.py`: Web utilities

- Game Modes:
  1. AI vs AI: Both describer and guesser are AI agents
  2. Human Describer vs AI Guesser: Human describes, AI guesses
  3. Human Guesser vs AI Describer: AI describes, human guesses

- State Management:
  - Game States (GameState Enum):
    - INITIALIZED: Game object created
    - ROUND_STARTING: Beginning of each round
    - DESCRIBING: Active description phase
    - GUESSING: Active guessing phase
    - ROUND_ENDED: Current round complete
    - GAME_OVER: All rounds complete

  - Round Results (RoundResult Enum):
    - CORRECT_GUESS: Successful guess
    - INCORRECT_GUESS: Wrong guess
    - TABOO_WORD_USED: Description violated rules
    - TIME_UP: Round time limit reached
    - SKIPPED: Player chose to skip

  - Persistent State:
    - Current round number
    - Score tracking
    - Round history
    - Time management
    - Word and taboo words
    - Description history
    - Previous guesses

- Input/Output Flow:
  1. Command Line Arguments:
     - Game mode selection
     - Round time limits
     - Number of rounds
     - Word length constraints
     - AI provider selection
  
  2. User Interaction:
     - Clear formatting with emojis and separators
     - Consistent status display (time, score, round)
     - Multi-turn conversation handling
     - Error handling and feedback
  
  3. State Transitions:
     - Mode-specific game loops
     - Skip and retry mechanisms
     - Round progression logic
     - Score and history tracking

- External Dependencies:
  - NLTK WordNet for word database
  - OpenAI GPT-4o for guessing
  - Anthropic Claude for describing
  - Python environment management
  - Standard libraries for CLI

## Debugging Lessons
- State Management:
  - Always explicitly set game state before critical operations
  - Validate state transitions with assertions or checks
  - Keep state management consistent across different modes
  - Document state flow in comments for complex transitions

- Error Handling:
  - Add descriptive error messages for each failure case
  - Include context in error messages (current state, expected state)
  - Log state transitions for debugging
  - Handle edge cases (timeouts, invalid inputs) gracefully

- Testing Strategy:
  - Test each mode independently before integration
  - Use working modes as reference for debugging
  - Compare state flows between different modes
  - Add debug logging for state transitions

- Code Organization:
  - Keep state management logic centralized
  - Use consistent patterns across similar features
  - Document complex state machines
  - Add debug information to stderr while keeping stdout clean

- User Experience:
  - Provide clear feedback for all actions
  - Show game state visually (emojis, formatting)
  - Make error messages actionable
  - Include context in prompts and feedback

- Description Handling:
  - Track all descriptions for current word in both game and agent
  - Clear description history at appropriate state transitions
  - Maintain proper order of descriptions for context
  - Handle edge cases (skips, timeouts) gracefully

- AI Interaction:
  - Structure prompts for clear progression of information
  - Adapt analysis based on attempt number
  - Provide context-aware feedback
  - Guide users toward more effective descriptions